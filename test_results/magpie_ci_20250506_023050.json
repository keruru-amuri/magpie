{
  "name": "MAGPIE CI",
  "timestamp": "2025-05-06T02:30:50.069563",
  "summary": {
    "name": "MAGPIE CI",
    "total_suites": 4,
    "total_results": 4,
    "status_counts": {
      "pending": 0,
      "running": 0,
      "passed": 0,
      "failed": 4,
      "skipped": 0,
      "error": 0
    },
    "pass_rate": 0.0,
    "total_duration": 18.103928327560425
  },
  "suites": [
    {
      "name": "Unit Tests",
      "type": "unit",
      "command": "python -m pytest tests/unit",
      "path": "tests/unit",
      "timeout": 300,
      "metadata": {},
      "results": [
        {
          "name": "Unit Tests",
          "type": "unit",
          "status": "failed",
          "duration": 4.955716371536255,
          "output": "============================= test session starts =============================\nplatform win32 -- Python 3.11.9, pytest-8.3.5, pluggy-1.5.0\nrootdir: C:\\Users\\2352650\\Documents\\augment-projects\\magpie\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, asyncio-0.26.0, cov-4.1.0\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollected 644 items / 3 errors\n\n=================================== ERRORS ====================================\n_______ ERROR collecting tests/unit/core/mock/test_mock_data_config.py ________\nImportError while importing test module 'C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\unit\\core\\mock\\test_mock_data_config.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\ntests\\unit\\core\\mock\\test_mock_data_config.py:8: in <module>\n    from app.core.mock.config import (\nE   ImportError: cannot import name 'is_mock_data_enabled' from 'app.core.mock.config' (C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\app\\core\\mock\\config.py)\n______ ERROR collecting tests/unit/core/mock/test_mock_data_generator.py ______\nImportError while importing test module 'C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\unit\\core\\mock\\test_mock_data_generator.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\ntests\\unit\\core\\mock\\test_mock_data_generator.py:9: in <module>\n    from app.core.mock.generator import (\nE   ImportError: cannot import name 'generate_documentation_data' from 'app.core.mock.generator' (C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\app\\core\\mock\\generator.py)\n_______ ERROR collecting tests/unit/core/mock/test_mock_data_loader.py ________\nImportError while importing test module 'C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\unit\\core\\mock\\test_mock_data_loader.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\ntests\\unit\\core\\mock\\test_mock_data_loader.py:9: in <module>\n    from app.core.mock.loader import (\nE   ImportError: cannot import name 'load_documentation_data' from 'app.core.mock.loader' (C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\app\\core\\mock\\loader.py)\n============================== warnings summary ===============================\n.venv\\Lib\\site-packages\\starlette\\formparsers.py:12\n  C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\.venv\\Lib\\site-packages\\starlette\\formparsers.py:12: PendingDeprecationWarning: Please use `import python_multipart` instead.\n    import multipart\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nERROR tests/unit/core/mock/test_mock_data_config.py\nERROR tests/unit/core/mock/test_mock_data_generator.py\nERROR tests/unit/core/mock/test_mock_data_loader.py\n!!!!!!!!!!!!!!!!!!! Interrupted: 3 errors during collection !!!!!!!!!!!!!!!!!!!\n======================== 1 warning, 3 errors in 1.14s =========================\n",
          "error": "C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:217: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n",
          "metadata": {},
          "timestamp": "2025-05-06T02:30:36.915981"
        }
      ]
    },
    {
      "name": "Integration Tests",
      "type": "integration",
      "command": "python -m pytest tests/integration",
      "path": "tests/integration",
      "timeout": 600,
      "metadata": {},
      "results": [
        {
          "name": "Integration Tests",
          "type": "integration",
          "status": "failed",
          "duration": 4.9293272495269775,
          "output": "============================= test session starts =============================\nplatform win32 -- Python 3.11.9, pytest-8.3.5, pluggy-1.5.0\nrootdir: C:\\Users\\2352650\\Documents\\augment-projects\\magpie\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, asyncio-0.26.0, cov-4.1.0\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollected 180 items / 3 errors\n\n=================================== ERRORS ====================================\n_____________ ERROR collecting tests/integration/api/test_auth.py _____________\ntests\\integration\\api\\test_auth.py:9: in <module>\n    from tests.conftest_auth import auth_client, db_session, fake_redis\ntests\\conftest_auth.py:31: in <module>\n    Base.metadata.create_all(bind=engine)\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:5825: in create_all\n    bind._run_ddl_visitor(\n.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3233: in _run_ddl_visitor\n    conn._run_ddl_visitor(visitorcallable, element, **kwargs)\n.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2439: in _run_ddl_visitor\n    visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\visitors.py:664: in traverse_single\n    return meth(obj, **kw)\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\ddl.py:896: in visit_metadata\n    collection = sort_tables_and_constraints(\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\ddl.py:1328: in sort_tables_and_constraints\n    dependent_on = fkc.referred_table\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:4742: in referred_table\n    return self.elements[0].column.table\n.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:1140: in __get__\n    obj.__dict__[self.__name__] = result = self.fget(obj)\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:3159: in column\n    return self._resolve_column()\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:3182: in _resolve_column\n    raise exc.NoReferencedTableError(\nE   sqlalchemy.exc.NoReferencedTableError: Foreign key associated with column 'documentreference.target_version_id' could not find table 'document_version' with which to generate a foreign key to target column 'id'\n------------------------------- Captured stderr -------------------------------\n\u001b[32m2025-05-05 18:30:40.116795+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m20744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.logging\u001b[0m:\u001b[36minitialize_logging\u001b[0m:\u001b[36m241\u001b[0m | \u001b[1mLogging initialized for MAGPIE in EnvironmentType.DEVELOPMENT mode\u001b[0m\u001b[32m2025-05-05 18:30:40.128107+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m20744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.monitoring.tracing\u001b[0m:\u001b[36mget_tracer_provider\u001b[0m:\u001b[36m160\u001b[0m | \u001b[1mDistributed tracing initialized for MAGPIE in EnvironmentType.DEVELOPMENT environment with 100.0% sampling\u001b[0m\u001b[32m2025-05-05 18:30:40.131113+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m20744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.monitoring.tracing\u001b[0m:\u001b[36msetup_tracing\u001b[0m:\u001b[36m385\u001b[0m | \u001b[1mLogging instrumentation enabled\u001b[0m\u001b[32m2025-05-05 18:30:40.174830+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m20744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.monitoring.tracing\u001b[0m:\u001b[36msetup_tracing\u001b[0m:\u001b[36m390\u001b[0m | \u001b[1mDistributed tracing set up successfully for MAGPIE with sampling ratio 1.00\u001b[0m\u001b[32m2025-05-05 18:30:40.174830+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m20744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.main\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m96\u001b[0m | \u001b[1mDistributed tracing initialized with OpenTelemetry\u001b[0m\n____________ ERROR collecting tests/integration/api/test_users.py _____________\ntests\\integration\\api\\test_users.py:28: in <module>\n    Base.metadata.create_all(bind=engine)\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:5825: in create_all\n    bind._run_ddl_visitor(\n.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3233: in _run_ddl_visitor\n    conn._run_ddl_visitor(visitorcallable, element, **kwargs)\n.venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2439: in _run_ddl_visitor\n    visitorcallable(self.dialect, self, **kwargs).traverse_single(element)\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\visitors.py:664: in traverse_single\n    return meth(obj, **kw)\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\ddl.py:896: in visit_metadata\n    collection = sort_tables_and_constraints(\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\ddl.py:1328: in sort_tables_and_constraints\n    dependent_on = fkc.referred_table\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:4742: in referred_table\n    return self.elements[0].column.table\n.venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:1140: in __get__\n    obj.__dict__[self.__name__] = result = self.fget(obj)\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:3159: in column\n    return self._resolve_column()\n.venv\\Lib\\site-packages\\sqlalchemy\\sql\\schema.py:3182: in _resolve_column\n    raise exc.NoReferencedTableError(\nE   sqlalchemy.exc.NoReferencedTableError: Foreign key associated with column 'documentreference.target_version_id' could not find table 'document_version' with which to generate a foreign key to target column 'id'\n_ ERROR collecting tests/integration/core/orchestrator/test_orchestrator_integration_framework.py _\nImportError while importing test module 'C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\integration\\core\\orchestrator\\test_orchestrator_integration_framework.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\ntests\\integration\\core\\orchestrator\\test_orchestrator_integration_framework.py:16: in <module>\n    from tests.framework.integration.environment import IntegrationTestEnvironment, DependencyType\ntests\\framework\\integration\\environment.py:15: in <module>\n    from app.core.db.connection import get_db, get_async_db\nE   ImportError: cannot import name 'get_async_db' from 'app.core.db.connection' (C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\app\\core\\db\\connection.py)\n============================== warnings summary ===============================\n.venv\\Lib\\site-packages\\starlette\\formparsers.py:12\n  C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\.venv\\Lib\\site-packages\\starlette\\formparsers.py:12: PendingDeprecationWarning: Please use `import python_multipart` instead.\n    import multipart\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nERROR tests/integration/api/test_auth.py - sqlalchemy.exc.NoReferencedTableEr...\nERROR tests/integration/api/test_users.py - sqlalchemy.exc.NoReferencedTableE...\nERROR tests/integration/core/orchestrator/test_orchestrator_integration_framework.py\n!!!!!!!!!!!!!!!!!!! Interrupted: 3 errors during collection !!!!!!!!!!!!!!!!!!!\n======================== 1 warning, 3 errors in 1.19s =========================\n",
          "error": "C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:217: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n",
          "metadata": {},
          "timestamp": "2025-05-06T02:30:41.845308"
        }
      ]
    },
    {
      "name": "Performance Tests",
      "type": "performance",
      "command": "python -m pytest tests/performance",
      "path": "tests/performance",
      "timeout": 1200,
      "metadata": {},
      "results": [
        {
          "name": "Performance Tests",
          "type": "performance",
          "status": "failed",
          "duration": 4.217513084411621,
          "output": "============================= test session starts =============================\nplatform win32 -- Python 3.11.9, pytest-8.3.5, pluggy-1.5.0\nrootdir: C:\\Users\\2352650\\Documents\\augment-projects\\magpie\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, asyncio-0.26.0, cov-4.1.0\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollected 12 items / 1 error\n\n=================================== ERRORS ====================================\n_ ERROR collecting tests/performance/core/orchestrator/test_orchestrator_performance.py _\nImportError while importing test module 'C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\performance\\core\\orchestrator\\test_orchestrator_performance.py'.\nHint: make sure your test modules/packages have valid Python names.\nTraceback:\ntests\\performance\\core\\orchestrator\\test_orchestrator_performance.py:15: in <module>\n    from tests.framework.performance.harness import PerformanceHarness\ntests\\framework\\performance\\harness.py:10: in <module>\n    import psutil\nE   ModuleNotFoundError: No module named 'psutil'\n------------------------------- Captured stderr -------------------------------\n\u001b[32m2025-05-05 18:30:45.196623+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m24952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.logging\u001b[0m:\u001b[36minitialize_logging\u001b[0m:\u001b[36m241\u001b[0m | \u001b[1mLogging initialized for MAGPIE in EnvironmentType.DEVELOPMENT mode\u001b[0m\u001b[32m2025-05-05 18:30:45.209101+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m24952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.monitoring.tracing\u001b[0m:\u001b[36mget_tracer_provider\u001b[0m:\u001b[36m160\u001b[0m | \u001b[1mDistributed tracing initialized for MAGPIE in EnvironmentType.DEVELOPMENT environment with 100.0% sampling\u001b[0m\u001b[32m2025-05-05 18:30:45.213219+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m24952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.monitoring.tracing\u001b[0m:\u001b[36msetup_tracing\u001b[0m:\u001b[36m385\u001b[0m | \u001b[1mLogging instrumentation enabled\u001b[0m\u001b[32m2025-05-05 18:30:45.256314+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m24952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.core.monitoring.tracing\u001b[0m:\u001b[36msetup_tracing\u001b[0m:\u001b[36m390\u001b[0m | \u001b[1mDistributed tracing set up successfully for MAGPIE with sampling ratio 1.00\u001b[0m\u001b[32m2025-05-05 18:30:45.256314+00:00\u001b[0m | \u001b[32mMAGPIE\u001b[0m | \u001b[32mPF5841WS\u001b[0m | \u001b[32m24952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mapp.main\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m96\u001b[0m | \u001b[1mDistributed tracing initialized with OpenTelemetry\u001b[0m\n============================== warnings summary ===============================\n.venv\\Lib\\site-packages\\starlette\\formparsers.py:12\n  C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\.venv\\Lib\\site-packages\\starlette\\formparsers.py:12: PendingDeprecationWarning: Please use `import python_multipart` instead.\n    import multipart\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nERROR tests/performance/core/orchestrator/test_orchestrator_performance.py\n!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n========================= 1 warning, 1 error in 0.35s =========================\n",
          "error": "C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:217: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n",
          "metadata": {},
          "timestamp": "2025-05-06T02:30:46.066837"
        }
      ]
    },
    {
      "name": "Quality Tests",
      "type": "quality",
      "command": "python -m pytest tests/quality",
      "path": "tests/quality",
      "timeout": 900,
      "metadata": {},
      "results": [
        {
          "name": "Quality Tests",
          "type": "quality",
          "status": "failed",
          "duration": 4.001371622085571,
          "output": "============================= test session starts =============================\nplatform win32 -- Python 3.11.9, pytest-8.3.5, pluggy-1.5.0\nrootdir: C:\\Users\\2352650\\Documents\\augment-projects\\magpie\nconfigfile: pytest.ini\nplugins: anyio-4.9.0, asyncio-0.26.0, cov-4.1.0\nasyncio: mode=Mode.AUTO, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function\ncollected 8 items\n\ntests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_documentation_agent_quality ERROR [ 12%]\ntests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_troubleshooting_agent_quality ERROR [ 25%]\ntests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_maintenance_agent_quality ERROR [ 37%]\ntests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_with_dimensions ERROR [ 50%]\ntests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_with_feedback_types ERROR [ 62%]\ntests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_with_edge_cases ERROR [ 75%]\ntests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_with_adversarial_cases ERROR [ 87%]\ntests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_batch_evaluation ERROR [100%]\n\n=================================== ERRORS ====================================\n_____ ERROR at setup of TestAgentQuality.test_documentation_agent_quality _____\nfile C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py, line 141\n      @pytest.mark.asyncio\n      async def test_documentation_agent_quality(\n          self,\n          documentation_agent,\n          evaluation_pipeline,\n          test_case_generator\n      ):\n          \"\"\"\n          Test documentation agent quality.\n\n          Args:\n              documentation_agent: Documentation agent\n              evaluation_pipeline: Evaluation pipeline\n              test_case_generator: Test case generator\n          \"\"\"\n          # Generate test cases\n          test_cases = test_case_generator.generate_test_cases(\n              count=3,\n              agent_types=[AgentType.DOCUMENTATION],\n              types=[TestCaseType.STANDARD]\n          )\n\n          # Process each test case\n          for test_case in test_cases:\n              # Process query\n              response = await documentation_agent.process_query(\n                  query=test_case.query,\n                  conversation_id=\"test-conversation\"\n              )\n\n              # Evaluate response\n              result = await evaluation_pipeline.evaluate(\n                  query=test_case.query,\n                  response=response[\"response\"],\n                  reference_dataset=\"aircraft_maintenance\"\n              )\n\n              # Verify quality\n              assert result.evaluation.overall_score is not None\n\n              # Verify feedback\n              assert len(result.feedback) > 0\nE       fixture 'documentation_agent' not found\n>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, async_client, async_orchestrator_client, async_override_get_db, async_test_engine, async_test_session, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, cov, dimension_generator, doctest_namespace, evaluation_pipeline, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_agent_repository, mock_azure_openai, mock_conversation_repository, mock_documentation_agent, mock_documentation_service, mock_llm_service, mock_maintenance_agent, mock_maintenance_service, mock_orchestrator, mock_troubleshooting_agent, mock_troubleshooting_service, monkeypatch, no_cover, orchestrator_client, override_get_db, override_settings, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reference_manager, test_agent_config, test_case_generator, test_conversation, test_db_engine, test_db_session, test_engine, test_environment, test_messages, test_session, test_user, tests/quality/core/agents/test_agent_quality.py::<event_loop>, tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::<event_loop>, tests::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nC:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py:141\n____ ERROR at setup of TestAgentQuality.test_troubleshooting_agent_quality ____\nfile C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py, line 184\n      @pytest.mark.asyncio\n      async def test_troubleshooting_agent_quality(\n          self,\n          troubleshooting_agent,\n          evaluation_pipeline,\n          test_case_generator\n      ):\n          \"\"\"\n          Test troubleshooting agent quality.\n\n          Args:\n              troubleshooting_agent: Troubleshooting agent\n              evaluation_pipeline: Evaluation pipeline\n              test_case_generator: Test case generator\n          \"\"\"\n          # Generate test cases\n          test_cases = test_case_generator.generate_test_cases(\n              count=3,\n              agent_types=[AgentType.TROUBLESHOOTING],\n              types=[TestCaseType.STANDARD]\n          )\n\n          # Process each test case\n          for test_case in test_cases:\n              # Process query\n              response = await troubleshooting_agent.process_query(\n                  query=test_case.query,\n                  conversation_id=\"test-conversation\"\n              )\n\n              # Evaluate response\n              result = await evaluation_pipeline.evaluate(\n                  query=test_case.query,\n                  response=response[\"response\"],\n                  reference_dataset=\"aircraft_maintenance\"\n              )\n\n              # Verify quality\n              assert result.evaluation.overall_score is not None\n\n              # Verify feedback\n              assert len(result.feedback) > 0\nE       fixture 'troubleshooting_agent' not found\n>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, async_client, async_orchestrator_client, async_override_get_db, async_test_engine, async_test_session, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, cov, dimension_generator, doctest_namespace, evaluation_pipeline, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_agent_repository, mock_azure_openai, mock_conversation_repository, mock_documentation_agent, mock_documentation_service, mock_llm_service, mock_maintenance_agent, mock_maintenance_service, mock_orchestrator, mock_troubleshooting_agent, mock_troubleshooting_service, monkeypatch, no_cover, orchestrator_client, override_get_db, override_settings, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reference_manager, test_agent_config, test_case_generator, test_conversation, test_db_engine, test_db_session, test_engine, test_environment, test_messages, test_session, test_user, tests/quality/core/agents/test_agent_quality.py::<event_loop>, tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::<event_loop>, tests::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nC:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py:184\n______ ERROR at setup of TestAgentQuality.test_maintenance_agent_quality ______\nfile C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py, line 227\n      @pytest.mark.asyncio\n      async def test_maintenance_agent_quality(\n          self,\n          maintenance_agent,\n          evaluation_pipeline,\n          test_case_generator\n      ):\n          \"\"\"\n          Test maintenance agent quality.\n\n          Args:\n              maintenance_agent: Maintenance agent\n              evaluation_pipeline: Evaluation pipeline\n              test_case_generator: Test case generator\n          \"\"\"\n          # Generate test cases\n          test_cases = test_case_generator.generate_test_cases(\n              count=3,\n              agent_types=[AgentType.MAINTENANCE],\n              types=[TestCaseType.STANDARD]\n          )\n\n          # Process each test case\n          for test_case in test_cases:\n              # Process query\n              response = await maintenance_agent.process_query(\n                  query=test_case.query,\n                  conversation_id=\"test-conversation\"\n              )\n\n              # Evaluate response\n              result = await evaluation_pipeline.evaluate(\n                  query=test_case.query,\n                  response=response[\"response\"],\n                  reference_dataset=\"aircraft_maintenance\"\n              )\n\n              # Verify quality\n              assert result.evaluation.overall_score is not None\n\n              # Verify feedback\n              assert len(result.feedback) > 0\nE       fixture 'maintenance_agent' not found\n>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, async_client, async_orchestrator_client, async_override_get_db, async_test_engine, async_test_session, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, cov, dimension_generator, doctest_namespace, evaluation_pipeline, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_agent_repository, mock_azure_openai, mock_conversation_repository, mock_documentation_agent, mock_documentation_service, mock_llm_service, mock_maintenance_agent, mock_maintenance_service, mock_orchestrator, mock_troubleshooting_agent, mock_troubleshooting_service, monkeypatch, no_cover, orchestrator_client, override_get_db, override_settings, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reference_manager, test_agent_config, test_case_generator, test_conversation, test_db_engine, test_db_session, test_engine, test_environment, test_messages, test_session, test_user, tests/quality/core/agents/test_agent_quality.py::<event_loop>, tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::<event_loop>, tests::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nC:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py:227\n____ ERROR at setup of TestAgentQuality.test_agent_quality_with_dimensions ____\nfile C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py, line 270\n      @pytest.mark.asyncio\n      async def test_agent_quality_with_dimensions(\n          self,\n          documentation_agent,\n          evaluation_pipeline,\n          test_case_generator,\n          dimension_generator\n      ):\n          \"\"\"\n          Test agent quality with specific dimensions.\n\n          Args:\n              documentation_agent: Documentation agent\n              evaluation_pipeline: Evaluation pipeline\n              test_case_generator: Test case generator\n              dimension_generator: Quality dimension generator\n          \"\"\"\n          # Generate dimensions and weights\n          dimensions, weights = dimension_generator.generate_weighted_dimensions(\n              count=3,\n              include_dimensions=[QualityDimension.RELEVANCE, QualityDimension.CLARITY]\n          )\n\n          # Generate test case\n          test_case = test_case_generator.generate_test_case(\n              agent_type=AgentType.DOCUMENTATION,\n              type=TestCaseType.STANDARD\n          )\n\n          # Process query\n          response = await documentation_agent.process_query(\n              query=test_case.query,\n              conversation_id=\"test-conversation\"\n          )\n\n          # Evaluate response\n          result = await evaluation_pipeline.evaluate(\n              query=test_case.query,\n              response=response[\"response\"],\n              reference_dataset=\"aircraft_maintenance\",\n              dimensions=dimensions,\n              weights=weights\n          )\n\n          # Verify quality\n          assert result.evaluation.overall_score is not None\n\n          # Verify dimensions\n          for dimension in dimensions:\n              assert result.evaluation.get_score(dimension) is not None\nE       fixture 'documentation_agent' not found\n>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, async_client, async_orchestrator_client, async_override_get_db, async_test_engine, async_test_session, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, cov, dimension_generator, doctest_namespace, evaluation_pipeline, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_agent_repository, mock_azure_openai, mock_conversation_repository, mock_documentation_agent, mock_documentation_service, mock_llm_service, mock_maintenance_agent, mock_maintenance_service, mock_orchestrator, mock_troubleshooting_agent, mock_troubleshooting_service, monkeypatch, no_cover, orchestrator_client, override_get_db, override_settings, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reference_manager, test_agent_config, test_case_generator, test_conversation, test_db_engine, test_db_session, test_engine, test_environment, test_messages, test_session, test_user, tests/quality/core/agents/test_agent_quality.py::<event_loop>, tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::<event_loop>, tests::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nC:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py:270\n__ ERROR at setup of TestAgentQuality.test_agent_quality_with_feedback_types __\nfile C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py, line 321\n      @pytest.mark.asyncio\n      async def test_agent_quality_with_feedback_types(\n          self,\n          documentation_agent,\n          evaluation_pipeline,\n          test_case_generator\n      ):\n          \"\"\"\n          Test agent quality with specific feedback types.\n\n          Args:\n              documentation_agent: Documentation agent\n              evaluation_pipeline: Evaluation pipeline\n              test_case_generator: Test case generator\n          \"\"\"\n          # Generate test case\n          test_case = test_case_generator.generate_test_case(\n              agent_type=AgentType.DOCUMENTATION,\n              type=TestCaseType.STANDARD\n          )\n\n          # Process query\n          response = await documentation_agent.process_query(\n              query=test_case.query,\n              conversation_id=\"test-conversation\"\n          )\n\n          # Evaluate response\n          result = await evaluation_pipeline.evaluate(\n              query=test_case.query,\n              response=response[\"response\"],\n              reference_dataset=\"aircraft_maintenance\",\n              feedback_types=[FeedbackType.RATING, FeedbackType.COMMENT]\n          )\n\n          # Verify feedback\n          assert len(result.feedback) > 0\n\n          # Verify feedback types\n          feedback_types = [feedback.type for feedback in result.feedback]\n          assert FeedbackType.RATING in feedback_types\n          assert FeedbackType.COMMENT in feedback_types\nE       fixture 'documentation_agent' not found\n>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, async_client, async_orchestrator_client, async_override_get_db, async_test_engine, async_test_session, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, cov, dimension_generator, doctest_namespace, evaluation_pipeline, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_agent_repository, mock_azure_openai, mock_conversation_repository, mock_documentation_agent, mock_documentation_service, mock_llm_service, mock_maintenance_agent, mock_maintenance_service, mock_orchestrator, mock_troubleshooting_agent, mock_troubleshooting_service, monkeypatch, no_cover, orchestrator_client, override_get_db, override_settings, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reference_manager, test_agent_config, test_case_generator, test_conversation, test_db_engine, test_db_session, test_engine, test_environment, test_messages, test_session, test_user, tests/quality/core/agents/test_agent_quality.py::<event_loop>, tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::<event_loop>, tests::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nC:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py:321\n____ ERROR at setup of TestAgentQuality.test_agent_quality_with_edge_cases ____\nfile C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py, line 364\n      @pytest.mark.asyncio\n      async def test_agent_quality_with_edge_cases(\n          self,\n          documentation_agent,\n          evaluation_pipeline,\n          test_case_generator\n      ):\n          \"\"\"\n          Test agent quality with edge cases.\n\n          Args:\n              documentation_agent: Documentation agent\n              evaluation_pipeline: Evaluation pipeline\n              test_case_generator: Test case generator\n          \"\"\"\n          # Generate edge case\n          edge_case = test_case_generator.generate_edge_case(\n              agent_type=AgentType.DOCUMENTATION\n          )\n\n          # Process query\n          response = await documentation_agent.process_query(\n              query=edge_case.query,\n              conversation_id=\"test-conversation\"\n          )\n\n          # Evaluate response\n          result = await evaluation_pipeline.evaluate(\n              query=edge_case.query,\n              response=response[\"response\"],\n              reference_dataset=\"aircraft_maintenance\"\n          )\n\n          # Verify quality\n          assert result.evaluation.overall_score is not None\nE       fixture 'documentation_agent' not found\n>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, async_client, async_orchestrator_client, async_override_get_db, async_test_engine, async_test_session, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, cov, dimension_generator, doctest_namespace, evaluation_pipeline, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_agent_repository, mock_azure_openai, mock_conversation_repository, mock_documentation_agent, mock_documentation_service, mock_llm_service, mock_maintenance_agent, mock_maintenance_service, mock_orchestrator, mock_troubleshooting_agent, mock_troubleshooting_service, monkeypatch, no_cover, orchestrator_client, override_get_db, override_settings, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reference_manager, test_agent_config, test_case_generator, test_conversation, test_db_engine, test_db_session, test_engine, test_environment, test_messages, test_session, test_user, tests/quality/core/agents/test_agent_quality.py::<event_loop>, tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::<event_loop>, tests::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nC:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py:364\n_ ERROR at setup of TestAgentQuality.test_agent_quality_with_adversarial_cases _\nfile C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py, line 400\n      @pytest.mark.asyncio\n      async def test_agent_quality_with_adversarial_cases(\n          self,\n          documentation_agent,\n          evaluation_pipeline,\n          test_case_generator\n      ):\n          \"\"\"\n          Test agent quality with adversarial cases.\n\n          Args:\n              documentation_agent: Documentation agent\n              evaluation_pipeline: Evaluation pipeline\n              test_case_generator: Test case generator\n          \"\"\"\n          # Generate adversarial case\n          adversarial_case = test_case_generator.generate_adversarial_case(\n              agent_type=AgentType.DOCUMENTATION\n          )\n\n          # Process query\n          response = await documentation_agent.process_query(\n              query=adversarial_case.query,\n              conversation_id=\"test-conversation\"\n          )\n\n          # Evaluate response\n          result = await evaluation_pipeline.evaluate(\n              query=adversarial_case.query,\n              response=response[\"response\"],\n              reference_dataset=\"aircraft_maintenance\"\n          )\n\n          # Verify quality\n          assert result.evaluation.overall_score is not None\n\n          # Verify safety score\n          safety_score = result.evaluation.get_score(QualityDimension.SAFETY)\n          if safety_score:\n              assert safety_score.score > 0\nE       fixture 'documentation_agent' not found\n>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, async_client, async_orchestrator_client, async_override_get_db, async_test_engine, async_test_session, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, cov, dimension_generator, doctest_namespace, evaluation_pipeline, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_agent_repository, mock_azure_openai, mock_conversation_repository, mock_documentation_agent, mock_documentation_service, mock_llm_service, mock_maintenance_agent, mock_maintenance_service, mock_orchestrator, mock_troubleshooting_agent, mock_troubleshooting_service, monkeypatch, no_cover, orchestrator_client, override_get_db, override_settings, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reference_manager, test_agent_config, test_case_generator, test_conversation, test_db_engine, test_db_session, test_engine, test_environment, test_messages, test_session, test_user, tests/quality/core/agents/test_agent_quality.py::<event_loop>, tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::<event_loop>, tests::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nC:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py:400\n___ ERROR at setup of TestAgentQuality.test_agent_quality_batch_evaluation ____\nfile C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py, line 441\n      @pytest.mark.asyncio\n      async def test_agent_quality_batch_evaluation(\n          self,\n          documentation_agent,\n          evaluation_pipeline,\n          test_case_generator\n      ):\n          \"\"\"\n          Test agent quality batch evaluation.\n\n          Args:\n              documentation_agent: Documentation agent\n              evaluation_pipeline: Evaluation pipeline\n              test_case_generator: Test case generator\n          \"\"\"\n          # Generate test cases\n          test_cases = test_case_generator.generate_test_cases(\n              count=3,\n              agent_types=[AgentType.DOCUMENTATION],\n              types=[TestCaseType.STANDARD]\n          )\n\n          # Process queries\n          queries = [test_case.query for test_case in test_cases]\n          responses = []\n\n          for query in queries:\n              response = await documentation_agent.process_query(\n                  query=query,\n                  conversation_id=\"test-conversation\"\n              )\n\n              responses.append(response[\"response\"])\n\n          # Evaluate responses\n          results = await evaluation_pipeline.evaluate_batch(\n              queries=queries,\n              responses=responses,\n              reference_dataset=\"aircraft_maintenance\",\n              concurrency=2\n          )\n\n          # Verify results\n          assert len(results) == len(queries)\n\n          for result in results:\n              assert result.evaluation.overall_score is not None\n              assert len(result.feedback) > 0\n\n          # Save results\n          evaluation_pipeline.save_results()\n\n          # Get average score\n          avg_score = evaluation_pipeline.get_average_score()\n          assert avg_score is not None\nE       fixture 'documentation_agent' not found\n>       available fixtures: _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, async_client, async_orchestrator_client, async_override_get_db, async_test_engine, async_test_session, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, client, cov, dimension_generator, doctest_namespace, evaluation_pipeline, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_agent_repository, mock_azure_openai, mock_conversation_repository, mock_documentation_agent, mock_documentation_service, mock_llm_service, mock_maintenance_agent, mock_maintenance_service, mock_orchestrator, mock_troubleshooting_agent, mock_troubleshooting_service, monkeypatch, no_cover, orchestrator_client, override_get_db, override_settings, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, reference_manager, test_agent_config, test_case_generator, test_conversation, test_db_engine, test_db_session, test_engine, test_environment, test_messages, test_session, test_user, tests/quality/core/agents/test_agent_quality.py::<event_loop>, tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::<event_loop>, tests::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory\n>       use 'pytest --fixtures [testpath]' for help on them.\n\nC:\\Users\\2352650\\Documents\\augment-projects\\magpie\\tests\\quality\\core\\agents\\test_agent_quality.py:441\n============================== warnings summary ===============================\n.venv\\Lib\\site-packages\\starlette\\formparsers.py:12\n  C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\.venv\\Lib\\site-packages\\starlette\\formparsers.py:12: PendingDeprecationWarning: Please use `import python_multipart` instead.\n    import multipart\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nERROR tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_documentation_agent_quality\nERROR tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_troubleshooting_agent_quality\nERROR tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_maintenance_agent_quality\nERROR tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_with_dimensions\nERROR tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_with_feedback_types\nERROR tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_with_edge_cases\nERROR tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_with_adversarial_cases\nERROR tests/quality/core/agents/test_agent_quality.py::TestAgentQuality::test_agent_quality_batch_evaluation\n======================== 1 warning, 8 errors in 0.17s =========================\n",
          "error": "C:\\Users\\2352650\\Documents\\augment-projects\\magpie\\.venv\\Lib\\site-packages\\pytest_asyncio\\plugin.py:217: PytestDeprecationWarning: The configuration option \"asyncio_default_fixture_loop_scope\" is unset.\nThe event loop scope for asynchronous fixtures will default to the fixture caching scope. Future versions of pytest-asyncio will default the loop scope for asynchronous fixtures to function scope. Set the default fixture loop scope explicitly in order to avoid unexpected behavior in the future. Valid fixture loop scopes are: \"function\", \"class\", \"module\", \"package\", \"session\"\n\n  warnings.warn(PytestDeprecationWarning(_DEFAULT_FIXTURE_LOOP_SCOPE_UNSET))\n",
          "metadata": {},
          "timestamp": "2025-05-06T02:30:50.068209"
        }
      ]
    }
  ]
}